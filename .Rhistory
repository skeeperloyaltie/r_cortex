# Load the necessary libraries and functions
library(gbm)  # Assuming we used the gbm package for modeling
library(readr)  # Assuming we use readRDS() function to load the test data
# Load the test data from Session 1 and Session 18
test_data_1 <- readRDS("test/test1.rds")
test_data_18 <- readRDS("test/test2.rds")
# Combine the test data from Session 1 and Session 18
test_data <- rbind(test_data_1, test_data_18)
# Preprocess the test data (if necessary) - perform any necessary transformations or feature engineering
head(test_data)
# Extract the input features from the test data
test_features <- test_data[, c("contrast_left", "contrast_right")]
head(test_features)
# Extract the input features from the test data
test_features <- test_data[, c("contrast_left", "contrast_right", "feedback_type")]
head(test_features)
# Make predictions using the trained model
test_predictions <- predict(model, newdata = test_features, type = "response")  # Adjust "trained_model" with the name of your trained model
# Make predictions using the trained model
test_predictions <- predict(model, newdata = test_features)  # Adjust "trained_model" with the name of your trained model
# Load required libraries
library(dplyr)
# Preparing the data for model training and prediction
# Select the relevant variables from the integrated data
selected_data <- integrated_data %>%
select(feedback_type, contrast_left, contrast_right, session)
# Convert feedback_type to a factor with two levels
selected_data$feedback_type <- factor(selected_data$feedback_type, levels = c(-1, 1), labels = c("failure", "success"))
# Split the data into training and testing sets
set.seed(123) # for reproducibility
train_indices <- sample(nrow(selected_data), nrow(selected_data) * 0.8) # 80% for training
train_data <- selected_data[train_indices, ]
test_data <- selected_data[-train_indices, ]
head(test_data)
# Build the prediction model
model <- train(
feedback_type ~ contrast_left + contrast_right + session,
data = train_data,
method = "gbm",  # Replace with the desired model method (e.g., "randomForest", "glm", etc.)
trControl = trainControl(method = "cv", number = 5), # Cross-validation with 5 folds
verbose = FALSE
)
knitr::opts_chunk$set(echo = TRUE)
# Load required libraries
library(readr)
# Define a function to perform exploratory data analysis for each session
perform_eda <- function(session_data) {
# Extract relevant variables
feedback_type <- session_data$feedback_type
contrast_left <- session_data$contrast_left
contrast_right <- session_data$contrast_right
spks <- session_data$spks
# Describe data structures
num_neurons <- nrow(spks)
num_trials <- length(feedback_type)
stimuli_conditions <- unique(paste(contrast_left, contrast_right))
feedback_types <- unique(feedback_type)
cat("Number of neurons:", num_neurons, "\n")
cat("Number of trials:", num_trials, "\n")
cat("Stimuli conditions:", stimuli_conditions, "\n")
cat("Feedback types:", feedback_types, "\n")
# Explore neural activities during each trial
for (i in 1:num_trials) {
# Further analyze the spike train data for each trial
spike_train <- spks[[i]]
# Perform desired analysis on spike_train
}
# Explore changes across trials
# Explore homogeneity and heterogeneity across sessions and mice
}
# Load the RDS files and perform exploratory data analysis for each session
session <- list()
for (i in 1:18) {
session[[i]] <- readRDS(paste('data/session', i, '.rds', sep=''))
cat("Session", i, "\n")
perform_eda(session[[i]])
}
# Create an empty data frame to store the integrated data
integrated_data <- data.frame()
# Combine data across sessions
for (i in 1:length(session)) {
# Extract relevant variables
feedback_type <- session[[i]]$feedback_type
contrast_left <- session[[i]]$contrast_left
contrast_right <- session[[i]]$contrast_right
spks <- session[[i]]$spks
# Create a data frame for the session
session_data <- data.frame(
feedback_type = feedback_type,
contrast_left = contrast_left,
contrast_right = contrast_right,
session = i
)
# Append the session data to the integrated data frame
integrated_data <- rbind(integrated_data, session_data)
}
# Print the integrated data frame
tail(integrated_data)
library(tidyverse)
library(caret)
# Load required libraries
library(dplyr)
# Preparing the data for model training and prediction
# Select the relevant variables from the integrated data
selected_data <- integrated_data %>%
select(feedback_type, contrast_left, contrast_right, session)
# Convert feedback_type to a factor with two levels
selected_data$feedback_type <- factor(selected_data$feedback_type, levels = c(-1, 1), labels = c("failure", "success"))
# Split the data into training and testing sets
set.seed(123) # for reproducibility
train_indices <- sample(nrow(selected_data), nrow(selected_data) * 0.8) # 80% for training
train_data <- selected_data[train_indices, ]
test_data <- selected_data[-train_indices, ]
head(test_data)
# Build the prediction model
model <- train(
feedback_type ~ contrast_left + contrast_right + session,
data = train_data,
method = "gbm",  # Replace with the desired model method (e.g., "randomForest", "glm", etc.)
trControl = trainControl(method = "cv", number = 5), # Cross-validation with 5 folds
verbose = FALSE
)
# Make predictions on the test set
predictions <- predict(model, newdata = test_data)
# Evaluate the performance of the model
accuracy <- confusionMatrix(predictions, test_data$feedback_type)$overall["Accuracy"]
cat("Model Accuracy:", accuracy, "\n")
# Load the necessary libraries and functions
library(gbm)  # Assuming we used the gbm package for modeling
library(readr)  # Assuming we use readRDS() function to load the test data
# Load the test data from Session 1 and Session 18
test_data_1 <- readRDS("test/test1.rds")
test_data_18 <- readRDS("test/test2.rds")
# Combine the test data from Session 1 and Session 18
test_data <- rbind(test_data_1, test_data_18)
# Preprocess the test data (if necessary) - perform any necessary transformations or feature engineering
head(test_data)
# Extract the input features from the test data
test_features <- test_data[, c("contrast_left", "contrast_right", "feedback_type")]
head(test_features)
# Make predictions using the trained model
test_predictions <- predict(model, newdata = test_features, type = "response")  # Adjust "trained_model" with the name of your trained model
# Extract the input features from the test data
test_features <- test_data[, c("contrast_left", "contrast_right", "feedback_type")]
test_features$feedback_type <- factor(test_features$feedback_type, levels = c(-1, 1), labels = c("failure", "success"))
head(test_features)
# Make predictions using the trained model
test_predictions <- predict(model, newdata = test_features, type = "response")  # Adjust "trained_model" with the name of your trained model
# Load required libraries
library(dplyr)
# Preparing the data for model training and prediction
# Select the relevant variables from the integrated data
selected_data <- integrated_data %>%
select(feedback_type, contrast_left, contrast_right, session)
# Convert feedback_type to a factor with two levels
selected_data$feedback_type <- factor(selected_data$feedback_type, levels = c(-1, 1), labels = c("failure", "success"))
head(selected_data)
# Extract the input features from the test data
test_features <- test_data %>%
select(feedback_type, contrast_left, contrast_right)
# Extract the input features from the test data
test_features <- test_data[, c("contrast_left", "contrast_right", "feedback_type")]
test_features$feedback_type <- factor(test_features$feedback_type, levels = c(-1, 1), labels = c("failure", "success"))
test
# Extract the input features from the test data
test_features <- test_data[, c("contrast_left", "contrast_right", "feedback_type")]
test_features$feedback_type <- factor(test_features$feedback_type, levels = c(-1, 1), labels = c("failure", "success"))
head(test_features)
head(test_features$feedback_type)
# Extract the input features from the test data
test_features <- test_data[, c("contrast_left", "contrast_right", "feedback_type")]
test_features$feedback_type <- factor(test_features$feedback_type, levels = c(-1, 1), labels = c("failure", "success"))
head(test_features$feedback_type)
# Make predictions using the trained model
test_predictions <- predict(model, newdata = test_features, type = "response")  # Adjust "trained_model" with the name of your trained model
# Make predictions using the trained model
test_predictions <- predict(model, newdata = test_features, type = "feedback_type")  # Adjust "trained_model" with the name of your trained model
# Evaluate the performance of the model
accuracy <- confusionMatrix(predictions, test_data$feedback_type)$overall["Accuracy"]
knitr::opts_chunk$set(echo = TRUE)
# Load required libraries
library(readr)
# Define a function to perform exploratory data analysis for each session
perform_eda <- function(session_data) {
# Extract relevant variables
feedback_type <- session_data$feedback_type
contrast_left <- session_data$contrast_left
contrast_right <- session_data$contrast_right
spks <- session_data$spks
# Describe data structures
num_neurons <- nrow(spks)
num_trials <- length(feedback_type)
stimuli_conditions <- unique(paste(contrast_left, contrast_right))
feedback_types <- unique(feedback_type)
cat("Number of neurons:", num_neurons, "\n")
cat("Number of trials:", num_trials, "\n")
cat("Stimuli conditions:", stimuli_conditions, "\n")
cat("Feedback types:", feedback_types, "\n")
# Explore neural activities during each trial
for (i in 1:num_trials) {
# Further analyze the spike train data for each trial
spike_train <- spks[[i]]
# Perform desired analysis on spike_train
}
# Explore changes across trials
# Explore homogeneity and heterogeneity across sessions and mice
}
# Load the RDS files and perform exploratory data analysis for each session
session <- list()
for (i in 1:18) {
session[[i]] <- readRDS(paste('data/session', i, '.rds', sep=''))
cat("Session", i, "\n")
perform_eda(session[[i]])
}
# Create an empty data frame to store the integrated data
integrated_data <- data.frame()
# Combine data across sessions
for (i in 1:length(session)) {
# Extract relevant variables
feedback_type <- session[[i]]$feedback_type
contrast_left <- session[[i]]$contrast_left
contrast_right <- session[[i]]$contrast_right
spks <- session[[i]]$spks
# Create a data frame for the session
session_data <- data.frame(
feedback_type = feedback_type,
contrast_left = contrast_left,
contrast_right = contrast_right,
session = i
)
# Append the session data to the integrated data frame
integrated_data <- rbind(integrated_data, session_data)
}
# Print the integrated data frame
tail(integrated_data)
library(tidyverse)
library(caret)
# Load required libraries
library(dplyr)
# Preparing the data for model training and prediction
# Select the relevant variables from the integrated data
selected_data <- integrated_data %>%
select(feedback_type, contrast_left, contrast_right, session)
# Convert feedback_type to a factor with two levels
selected_data$feedback_type <- factor(selected_data$feedback_type, levels = c(-1, 1), labels = c("failure", "success"))
head(selected_data)
# Build the prediction model
model <- train(
feedback_type ~ contrast_left + contrast_right + session,
data = train_data,
method = "gbm",  # Replace with the desired model method (e.g., "randomForest", "glm", etc.)
trControl = trainControl(method = "cv", number = 5), # Cross-validation with 5 folds
verbose = FALSE
)
# Make predictions on the test set
predictions <- predict(model, newdata = test_data)
# Split the data into training and testing sets
set.seed(123) # for reproducibility
train_indices <- sample(nrow(selected_data), nrow(selected_data) * 0.8) # 80% for training
train_data <- selected_data[train_indices, ]
test_data <- selected_data[-train_indices, ]
head(test_data)
# Build the prediction model
model <- train(
feedback_type ~ contrast_left + contrast_right + session,
data = train_data,
method = "gbm",  # Replace with the desired model method (e.g., "randomForest", "glm", etc.)
trControl = trainControl(method = "cv", number = 5), # Cross-validation with 5 folds
verbose = FALSE
)
# Make predictions on the test set
predictions <- predict(model, newdata = test_data)
# Evaluate the performance of the model
accuracy <- confusionMatrix(predictions, test_data$feedback_type)$overall["Accuracy"]
cat("Model Accuracy:", accuracy, "\n")
# Load the necessary libraries and functions
library(gbm)  # Assuming we used the gbm package for modeling
library(readr)  # Assuming we use readRDS() function to load the test data
# Load the test data from Session 1 and Session 18
test_data_1 <- readRDS("test/test1.rds")
test_data_18 <- readRDS("test/test2.rds")
# Combine the test data from Session 1 and Session 18
test_data <- rbind(test_data_1, test_data_18)
# Preprocess the test data (if necessary) - perform any necessary transformations or feature engineering
head(test_data)
# Extract the input features from the test data
test_features <- test_data[, c("contrast_left", "contrast_right", "feedback_type")]
test_features$feedback_type <- factor(test_features$feedback_type, levels = c(-1, 1), labels = c("failure", "success"))
head(test_features$feedback_type)
knitr::opts_chunk$set(echo = TRUE)
# Load required libraries
library(readr)
# Define a function to perform exploratory data analysis for each session
perform_eda <- function(session_data) {
# Extract relevant variables
feedback_type <- session_data$feedback_type
contrast_left <- session_data$contrast_left
contrast_right <- session_data$contrast_right
spks <- session_data$spks
# Describe data structures
num_neurons <- nrow(spks)
num_trials <- length(feedback_type)
stimuli_conditions <- unique(paste(contrast_left, contrast_right))
feedback_types <- unique(feedback_type)
cat("Number of neurons:", num_neurons, "\n")
cat("Number of trials:", num_trials, "\n")
cat("Stimuli conditions:", stimuli_conditions, "\n")
cat("Feedback types:", feedback_types, "\n")
# Explore neural activities during each trial
for (i in 1:num_trials) {
# Further analyze the spike train data for each trial
spike_train <- spks[[i]]
# Perform desired analysis on spike_train
}
# Explore changes across trials
# Explore homogeneity and heterogeneity across sessions and mice
}
# Load the RDS files and perform exploratory data analysis for each session
session <- list()
for (i in 1:18) {
session[[i]] <- readRDS(paste('data/session', i, '.rds', sep=''))
cat("Session", i, "\n")
perform_eda(session[[i]])
}
# Create an empty data frame to store the integrated data
integrated_data <- data.frame()
# Combine data across sessions
for (i in 1:length(session)) {
# Extract relevant variables
feedback_type <- session[[i]]$feedback_type
contrast_left <- session[[i]]$contrast_left
contrast_right <- session[[i]]$contrast_right
spks <- session[[i]]$spks
# Create a data frame for the session
session_data <- data.frame(
feedback_type = feedback_type,
contrast_left = contrast_left,
contrast_right = contrast_right,
session = i
)
# Append the session data to the integrated data frame
integrated_data <- rbind(integrated_data, session_data)
}
# Print the integrated data frame
tail(integrated_data)
library(tidyverse)
library(caret)
# Load required libraries
library(dplyr)
# Preparing the data for model training and prediction
# Select the relevant variables from the integrated data
selected_data <- integrated_data %>%
select(feedback_type, contrast_left, contrast_right, session)
# Convert feedback_type to a factor with two levels
selected_data$feedback_type <- factor(selected_data$feedback_type, levels = c(-1, 1), labels = c("failure", "success"))
head(selected_data)
# Split the data into training and testing sets
set.seed(123) # for reproducibility
train_indices <- sample(nrow(selected_data), nrow(selected_data) * 0.8) # 80% for training
train_data <- selected_data[train_indices, ]
test_data <- selected_data[-train_indices, ]
head(test_data)
# Build the prediction model
model <- train(
feedback_type ~ contrast_left + contrast_right + session,
data = train_data,
method = "gbm",  # Replace with the desired model method (e.g., "randomForest", "glm", etc.)
trControl = trainControl(method = "cv", number = 5), # Cross-validation with 5 folds
verbose = FALSE
)
# Make predictions on the test set
predictions <- predict(model, newdata = test_data)
# Evaluate the performance of the model
accuracy <- confusionMatrix(predictions, test_data$feedback_type)$overall["Accuracy"]
cat("Model Accuracy:", accuracy, "\n")
# Load the necessary libraries and functions
library(gbm)  # Assuming we used the gbm package for modeling
library(readr)  # Assuming we use readRDS() function to load the test data
# Load the test data from Session 1 and Session 18
test_data_1 <- readRDS("test/test1.rds")
test_data_18 <- readRDS("test/test2.rds")
# Combine the test data from Session 1 and Session 18
test_data_ <- rbind(test_data_1, test_data_18)
# Preprocess the test data (if necessary) - perform any necessary transformations or feature engineering
head(test_data)
# Extract the input features from the test data
test_features <- test_data_[, c("contrast_left", "contrast_right", "feedback_type")]
test_features$feedback_type <- factor(test_features$feedback_type, levels = c(-1, 1), labels = c("failure", "success"))
head(test_features$feedback_type)
# Make predictions using the trained model
test_predictions <- predict(model, newdata = test_data, type = "response")  # Adjust "trained_model" with the name of your trained model
# Make predictions using the trained model
test_predictions <- predict(model, newdata = test_data)  # Adjust "trained_model" with the name of your trained model
# Assess the performance of the predictions based on the chosen criteria
# Define your evaluation metrics and compare the predicted outcomes with the true feedback types in the test data
# Example of evaluation metric: Accuracy
# Assuming the true feedback types are stored in a variable called "true_feedback_types" in the test data
true_feedback_types <- test_data$feedback_type
accuracy <- sum(test_predictions == true_feedback_types) / length(true_feedback_types)
# Print the performance metric(s)
print(paste("Accuracy:", accuracy))
knitr::opts_chunk$set(echo = TRUE)
# Load required libraries
library(readr)
# Define a function to perform exploratory data analysis for each session
perform_eda <- function(session_data) {
# Extract relevant variables
feedback_type <- session_data$feedback_type
contrast_left <- session_data$contrast_left
contrast_right <- session_data$contrast_right
spks <- session_data$spks
# Describe data structures
num_neurons <- nrow(spks)
num_trials <- length(feedback_type)
stimuli_conditions <- unique(paste(contrast_left, contrast_right))
feedback_types <- unique(feedback_type)
cat("Number of neurons:", num_neurons, "\n")
cat("Number of trials:", num_trials, "\n")
cat("Stimuli conditions:", stimuli_conditions, "\n")
cat("Feedback types:", feedback_types, "\n")
# Explore neural activities during each trial
for (i in 1:num_trials) {
# Further analyze the spike train data for each trial
spike_train <- spks[[i]]
# Perform desired analysis on spike_train
}
# Explore changes across trials
# Explore homogeneity and heterogeneity across sessions and mice
}
# Load the RDS files and perform exploratory data analysis for each session
session <- list()
for (i in 1:18) {
session[[i]] <- readRDS(paste('data/session', i, '.rds', sep=''))
cat("Session", i, "\n")
perform_eda(session[[i]])
}
# Create an empty data frame to store the integrated data
integrated_data <- data.frame()
# Combine data across sessions
for (i in 1:length(session)) {
# Extract relevant variables
feedback_type <- session[[i]]$feedback_type
contrast_left <- session[[i]]$contrast_left
contrast_right <- session[[i]]$contrast_right
spks <- session[[i]]$spks
# Create a data frame for the session
session_data <- data.frame(
feedback_type = feedback_type,
contrast_left = contrast_left,
contrast_right = contrast_right,
session = i
)
# Append the session data to the integrated data frame
integrated_data <- rbind(integrated_data, session_data)
}
# Print the integrated data frame
tail(integrated_data)
library(tidyverse)
library(caret)
# Load required libraries
library(dplyr)
# Preparing the data for model training and prediction
# Select the relevant variables from the integrated data
selected_data <- integrated_data %>%
select(feedback_type, contrast_left, contrast_right, session)
# Convert feedback_type to a factor with two levels
selected_data$feedback_type <- factor(selected_data$feedback_type, levels = c(-1, 1), labels = c("failure", "success"))
head(selected_data)
# Split the data into training and testing sets
set.seed(123) # for reproducibility
train_indices <- sample(nrow(selected_data), nrow(selected_data) * 0.8) # 80% for training
train_data <- selected_data[train_indices, ]
test_data <- selected_data[-train_indices, ]
head(test_data)
# Build the prediction model
model <- train(
feedback_type ~ contrast_left + contrast_right + session,
data = train_data,
method = "gbm",  # Replace with the desired model method (e.g., "randomForest", "glm", etc.)
trControl = trainControl(method = "cv", number = 5), # Cross-validation with 5 folds
verbose = FALSE
)
# Make predictions on the test set
predictions <- predict(model, newdata = test_data)
# Evaluate the performance of the model
accuracy <- confusionMatrix(predictions, test_data$feedback_type)$overall["Accuracy"]
cat("Model Accuracy:", accuracy, "\n")
# Load the necessary libraries and functions
library(gbm)  # Assuming we used the gbm package for modeling
library(readr)  # Assuming we use readRDS() function to load the test data
# Load the test data from Session 1 and Session 18
test_data_1 <- readRDS("test/test1.rds")
test_data_18 <- readRDS("test/test2.rds")
# Combine the test data from Session 1 and Session 18
test_data_ <- rbind(test_data_1, test_data_18)
# Preprocess the test data (if necessary) - perform any necessary transformations or feature engineering
head(test_data)
# Extract the input features from the test data
test_features <- test_data_[, c("contrast_left", "contrast_right", "feedback_type")]
test_features$feedback_type <- factor(test_features$feedback_type, levels = c(-1, 1), labels = c("failure", "success"))
head(test_features$feedback_type)
# Make predictions using the trained model
test_predictions <- predict(model, newdata = test_data)  # Adjust "trained_model" with the name of your trained model
# Assess the performance of the predictions based on the chosen criteria
# Define your evaluation metrics and compare the predicted outcomes with the true feedback types in the test data
# Example of evaluation metric: Accuracy
# Assuming the true feedback types are stored in a variable called "true_feedback_types" in the test data
true_feedback_types <- test_data$feedback_type
accuracy <- sum(test_predictions == true_feedback_types) / length(true_feedback_types)
# Print the performance metric(s)
print(paste("Accuracy:", accuracy))
sessionInfo()
